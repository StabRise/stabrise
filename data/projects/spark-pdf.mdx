---
title: Spark-PDF
imgSrc: /static/images/projects/sparkpdf.webp
description: An Open-Source Data Source for dealing with PDF files in Apache Spark
href: /projects/spark-pdf/
colab: https://colab.research.google.com/github/StabRise/spark-pdf/blob/main/examples/PdfDataSource.ipynb
test: https://github.com/StabRise/spark-pdf/actions/
maven: https://search.maven.org/artifact/com.stabrise/spark-pdf-spark35_2.12
license: https://github.com/StabRise/spark-pdf/blob/master/LICENSE
codacy: https://app.codacy.com/gh/StabRise/spark-pdf/dashboard?utm_source=gh&amp;utm_medium=referral&amp;utm_content=&amp;utm_campaign=Badge_grade
pypi:
pStabRise:
github: https://github.com/StabRise/spark-pdf
gettingStarted: https://github.com/StabRise/spark-pdf/blob/main/examples/PdfDataSource.ipynb
---

# PDF to Spark DataFrame: A Custom Data Source for Apache Spark

This project provides a custom data source for Apache Spark, enabling you to read PDF files directly
into Spark DataFrames. Itâ€™s designed to simplify the process of working with PDFs in distributed
data pipelines, whether you're dealing with text-based documents, scanned PDFs, or large files
with thousands of pages.

import Note from "@/components/Note";

<Note
  title="Databricks Integration"
  icon="ðŸš€"
>
 This project now works on Databricks. Check out the [Databricks example](https://github.com/StabRise/spark-pdf/blob/main/examples/PdfDataSourceDatabricks.ipynb)
 for more details.
</Note>

## Key Features

- **Read PDFs into DataFrames**: Directly load PDF files into Spark DataFrames
- **Lazy Loading**: Process PDFs page by page to optimize memory usage and handle large files efficiently.
- **Scala and Python Support**: Use the library with both Scala and PySpark APIs.
- **Built-in OCR**: Extract text from scanned PDFs using integrated OCRâ€”no need to install or configure Tesseract separately.
- **Large File Support**: Handle PDFs with up to 10,000 pages without performance bottlenecks.
- **Spark Connect Compatibility**: Works seamlessly with Spark Connect for distributed processing.

<Note
  title="ScaleDP Compatibility"
  icon="ðŸ‘‰"
>
  Compatible with [ScaleDP, an Open-Source Library](/projects/scale-dp) for Processing
  Documents using AI/ML in Apache Spark.
</Note>
>

## How It Works

The library extends Apache Sparkâ€™s data source API, allowing you to treat PDFs as a native data source.
For text-based PDFs, it extracts content directly.
For scanned PDFs, the built-in OCR engine processes the images and extracts text.
The lazy loading feature ensures that only the required pages are loaded into memory, making it efficient for
large files.

## Getting Started

### Requirements:

- `Java` 8, 11, 17
- `Apache Spark` 3.3.2, 3.4.1, 3.5.0, 4.0.0
- `Ghostscript` 9.50 or later (only for the GhostScript reader)

<Note
  title="Support of `Spark` 4.0.0"
  icon="ðŸ‘‰"
>
  `Spark` 4.0.0 is supported in the version 0.1.15 and later (requires Java 17 and Scala 2.13).
</Note>
>

## Installation

The binary package is available in the Maven Central Repository.
To install the package for your version of Apache Spark, use the
following Maven coordinates:

- **For Spark 3.5**: `com.stabrise:spark-pdf-spark35_2.12:0.1.15`
- **For Spark 3.4**: `com.stabrise:spark-pdf-spark34_2.12:0.1.11`
- **For Spark 3.3**: `com.stabrise:spark-pdf-spark33_2.12:0.1.15`
- **For Spark 4.0**: `com.stabrise:spark-pdf-spark40_2.13:0.1.15`

Simply add the corresponding dependency to your projectâ€™s `pom.xml` or build configuration.

### Options for the data source:

- `imageType`: Oputput image type. Can be: *"BINARY", "GREY", "RGB"*. Default: *"RGB".
- `resolution`: Resolution for rendering PDF page to the image. Default: "300" dpi.
- `pagePerPartition`: Number pages per partition in Spark DataFrame. Default: "5".
- `reader:` Supports: pdfBox - based on PdfBox java lib, gs - based on GhostScript (need installation GhostScipt to the system)

### Output Columns in the DataFrame

The DataFrame contains the following columns:

- **`path`**: The path to the PDF file.
- **`page_number`**: The page number within the document.
- **`text`**: The extracted text from the text layer of the PDF page.
- **`image`**: The image representation of the page.
- **`document`**: The OCR-extracted text from the rendered image (calls Tesseract OCR).
- **`partition_number`**: The partition number.

### Example of usage

#### Scala Example

The following Scala code demonstrates how to read a PDF file into a Spark DataFrame. It sets various options like image type, resolution, pages per partition, and the reader to use (PdfBox in this case):

```scala:Scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
.appName("Spark PDF Example")
.master("local[*]")
.config("spark.jars.packages", "com.stabrise:spark-pdf_2.12:0.1.15")
.getOrCreate()

val df = spark.read.format("pdf")
.option("imageType", "BINARY")
.option("resolution", "200")
.option("pagePerPartition", "2")
.option("reader", "pdfBox")
.load("path to the pdf file(s)")

df.select("path", "document").show()
```

#### Python Example

In the Python example, the process is similar, where we use PySpark to load a PDF into a DataFrame.
This code shows how to configure the same options for the pdf data source:

```python:Python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
.master("local[*]") \
.appName("SparkPdf") \
.config("spark.jars.packages", "com.stabrise:spark-pdf_2.12:0.1.15") \
.getOrCreate()

df = spark.read.format("pdf") \
.option("imageType", "BINARY") \
.option("resolution", "200") \
.option("pagePerPartition", "2") \
.option("reader", "pdfBox") \
.load("path to the pdf file(s)")

df.select("path", "document").show()
```